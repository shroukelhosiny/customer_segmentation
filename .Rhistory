Q1 <- quantile(df$Annual.Income..k.., .25)
Q3 <- quantile(df$Annual.Income..k.., .75)
IQR <- IQR(df$Annual.Income..k..)
#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3
df <- subset(df, df$Annual.Income..k..> (Q1 - 1.5*IQR) & df$Annual.Income..k..< (Q3 + 1.5*IQR))
#to check outlier is remover
boxplot(df)
# df data frame for clustering algorithms
#df2  data frame for classification algorithms
df2 <- df
#create column to spend score range (low, med,high)
df2[, 'spend_score_range'] = NA
df2$spend_score_range[df2$Spending.Score..1.100.<35]<-"low"
df2$spend_score_range[df2$Spending.Score..1.100.>=35&df2$Spending.Score..1.100.<70]<-"med"
df2$spend_score_range[df2$Spending.Score..1.100.>=70]<-"high"
spend_unique<-unique(df2$spend_score_range)
#convert categorical data to numeric
df2$spend_score_range<-as.numeric(factor(df2$spend_score_range,levels = spend_unique))
#to drop spending score from data frame that using for classification algorithms
df2 <- subset(df2 , select = -c(Spending.Score..1.100.))
summary(df)
range(df$Age)
range(df$Annual.Income..k..)
range(df$Spending.Score..1.100.)
sd(df$Age)
sd(df$Annual.Income..k..)
sd(df$Spending.Score..1.100.)
var(df$Age)
var(df$Annual.Income..k..)
var(df$Spending.Score..1.100.)
IQR(df$Age)
IQR(df$Annual.Income..k..)
IQR(df$Spending.Score..1.100.)
# +ve weak  correlation
cor(df$Annual.Income..k.. ,df$Spending.Score..1.100.)
mean(df$Annual.Income..k..)
median(df$Annual.Income..k..)
# Mean less than Median so it is Negative skewed
plot(density(df$Annual.Income..k..))   #Right Skewed
#naive Bayes
# Splitting data into train & test
split <- sample.split(df2, SplitRatio = 0.8)
train_cl_b <- subset(df2, split == "TRUE")
test_cl_b <- subset(df2, split == "FALSE")
# Fitting Naive Bayes Model
# to training dataset
set.seed(120)  # Setting Seed
classifier_cl <- naiveBayes(spend_score_range ~ ., data = train_cl_b)
classifier_cl
# Predicting on test data'
y_pred <- predict(classifier_cl, newdata = test_cl_b)
y_pred
# Confusion Matrix
cm <- table(test_cl_b$spend_score_range, y_pred)
cm
#Decision Tree
#split into train and test data
sample_data = sample.split(df2, SplitRatio = 0.8)
train_data <- subset(df2, sample_data == TRUE)
test_data <- subset(df2, sample_data == FALSE)
model<- ctree(spend_score_range ~ ., train_data)
plot(model)
#predict
predict_model<-predict(model, test_data)
m_at <- table(test_data$spend_score_range, predict_model)
m_at
#calculate accuracy
ac_Test <- sum(diag(m_at)) / sum(m_at)
print(paste('Accuracy for test is found to be',ac_Test))
#KNN
# Splitting data into train
# and test data
split_k <- sample.split(df2, SplitRatio = 0.8)
train_cl_k <- subset(df2, split_k == TRUE)
test_cl_k <- subset(df2, split_k == FALSE)
# to training dataset
classifier_knn <- knn(train = train_cl_k ,test = test_cl_k,cl = train_cl_k$Gender, k = 1)
# Confusiin Matrix
cm <- table(test_cl_k$Gender, classifier_knn)
cm
# Model Evaluation - Choosing K
# Calculate out of Sample error
misClassError <- mean(classifier_knn != test_cl_k$Gender)
print(paste('Accuracy =', 1-misClassError))
# K = 3
classifier_knn <- knn(train = train_cl_k ,test = test_cl_k,cl = train_cl_k$Gender, k = 3)
misClassError <- mean(classifier_knn != test_cl_k$Gender)
print(paste('Accuracy =', 1-misClassError))
# K = 5
classifier_knn <- knn(train = train_cl_k ,test = test_cl_k,cl = train_cl_k$Gender, k = 5)
misClassError <- mean(classifier_knn != test_cl_k$Gender)
print(paste('Accuracy =', 1-misClassError))
# K = 7
classifier_knn <- knn(train = train_cl_k ,test = test_cl_k,cl = train_cl_k$Gender, k = 7)
misClassError <- mean(classifier_knn != test_cl_k$Gender)
print(paste('Accuracy =', 1-misClassError))
setwd("~/data set")
# df data frame for clustering algorithms
#df2  data frame for classification algorithms
df2 <- df
#create column to spend score range (low, med,high)
df2[, 'spend_score_range'] = NA
df2$spend_score_range[df2$Spending.Score..1.100.<35]<-"low"
df2$spend_score_range[df2$Spending.Score..1.100.>=35&df2$Spending.Score..1.100.<70]<-"med"
df2$spend_score_range[df2$Spending.Score..1.100.>=70]<-"high"
spend_unique<-unique(df2$spend_score_range)
#convert categorical data to numeric
df2$spend_score_range<-as.numeric(factor(df2$spend_score_range,levels = spend_unique))
#to drop spending score from data frame that using for classification algorithms
df2 <- subset(df2 , select = -c(Spending.Score..1.100.))
######################display Pie Chart Between Genders###################################
#count number of males and females
maleCount<-length(which(df2$Gender=="Male"))
femaleCount<-length(which(df2$Gender=="Female"))
#store it into list(combine)
gendercount<-c(maleCount,femaleCount)
genderLabel<-c("Male","Female")
#get the percentage of each gender
GenderPercent<- round(100 * gendercount / sum(gendercount), 1)
#display the pie chart and percentage
pie(gendercount,label=GenderPercent, main = "Gender pie chart",, col = rainbow(length(gendercount)))
#display the color of gender
legend("topright",genderLabel,
cex = 0.9, fill = rainbow(length(gendercount)))
######################display Pie Chart Between Spending Scores###################################
lowCount<-length(which(df2$spend_score_range=="low"))
midCount<-length(which(df2$spend_score_range=="med"))
highCount<-length(which(df2$spend_score_range=="high"))
Rangecount<-c(lowCount,midCount,highCount)
rangeLabel<-c("low [1:35]","mid [35:70]","high [70:100]")
RangePercent<- round(100 * Rangecount / sum(Rangecount), 1)
pie(Rangecount,label=RangePercent, main = "Spending Score Range pie chart",, col = rainbow(length(Rangecount)))
legend("topright",rangeLabel,
cex = 0.9, fill = rainbow(length(Rangecount)))
#setwd('E:/third year material/firstSemester/Statistical Inference/Team_code')
#read data set and put in data frame
df<-read.csv("CustomerSegmentation.csv")
#summary about the data set
str(df)
#drop id CustomerID we don't need it
df <- subset(df , select = -c(CustomerID))
#check if exist missing values
complete.cases(df)
#to remove any record contain missing values
#df<- df[complete.cases(df), ]
#check if exist duplicated rows
duplicated(df)
#to remove duplicated rows
df<-df[!duplicated(df), ]
#Get A copy of Clean Data Frame To Use it in end of file without transforming Gender Column
Kmeans_DF<-df
#convert categorical variables to numirc
df$Gender=(factor(df$Gender , level =c("Male" , "Female") , labels =c(0,1) ))
#keep data with age more than or equal 18
df=df[df$Age>=18,]
#keep data with income more than or equal 0
df=df[df$Annual.Income..k..>=0,]
#keaap data with spending score more than or equal 1 (delete any score row less than 1)
df=df[df$Spending.Score..1.100.>=1,]
#keap data with spending score less than or equal 100 ((delete any score row more than 100))
df=df[df$Spending.Score..1.100.<=100,]
#exist outlier in Annual.Income column
#outlier = any value > (Q3+1.5*IQR) or any value < (Q1-1.5*IQR)
boxplot(df)
#boxplot(df$Annual.Income..k..)$out
#find Q1, Q3, and interquartile range for values in column Annual.Income
Q1 <- quantile(df$Annual.Income..k.., .25)
Q3 <- quantile(df$Annual.Income..k.., .75)
IQR <- IQR(df$Annual.Income..k..)
#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3
df <- subset(df, df$Annual.Income..k..> (Q1 - 1.5*IQR) & df$Annual.Income..k..< (Q3 + 1.5*IQR))
#to check outlier is remover
boxplot(df)
# df data frame for clustering algorithms
#df2  data frame for classification algorithms
df2 <- df
#create column to spend score range (low, med,high)
df2[, 'spend_score_range'] = NA
df2$spend_score_range[df2$Spending.Score..1.100.<35]<-"low"
df2$spend_score_range[df2$Spending.Score..1.100.>=35&df2$Spending.Score..1.100.<70]<-"med"
df2$spend_score_range[df2$Spending.Score..1.100.>=70]<-"high"
spend_unique<-unique(df2$spend_score_range)
#convert categorical data to numeric
df2$spend_score_range<-as.numeric(factor(df2$spend_score_range,levels = spend_unique))
#to drop spending score from data frame that using for classification algorithms
df2 <- subset(df2 , select = -c(Spending.Score..1.100.))
###############################Statistics#################################################
summary(df)
range(df$Age)
range(df$Annual.Income..k..)
range(df$Spending.Score..1.100.)
sd(df$Age)
sd(df$Annual.Income..k..)
sd(df$Spending.Score..1.100.)
var(df$Age)
var(df$Annual.Income..k..)
var(df$Spending.Score..1.100.)
IQR(df$Age)
IQR(df$Annual.Income..k..)
IQR(df$Spending.Score..1.100.)
# +ve weak  correlation
cor(df$Annual.Income..k.. ,df$Spending.Score..1.100.)
mean(df$Annual.Income..k..)
median(df$Annual.Income..k..)
# Mean less than Median so it is Negative skewed
plot(density(df$Annual.Income..k..))   #Right Skewed
##################################Visualization################################################
# 1-> Prepare Data that create Visualization on it
df <- as_tibble(df)  #Shape Of Data Frame
preparData <- ggplot(df)
preparData                                                  #delete
# 2 -> Aesthetics (Mapping Data for (x-axis & y-axis))
preparData <- ggplot(df,aes(Gender,Spending.Score..1.100.))
preparData
# 3 -> Geometrics
preparData + geom_point() #Make Scatter Plot
###conclusion:: Unuseful graph##
#######################
# Show Data on Histrogram
# Male --> 0
# Female --> 1
# 1-> Prepare Data that create Visualization on it
customerData_histrogram <- df
customerData_histrogram
# 2 -> Aesthetics (Mapping Data for (x-axis & y-axis))
data_histrogram <- ggplot(data = customerData_histrogram,aes(x =Spending.Score..1.100. ,fill=Gender))
data_histrogram
data_histrogram + geom_histogram(binwidth = 5, alpha=0.5) +
ggtitle("Spending Score of customers") +
labs(y = "Number Of customer",x="Spending Score")
###conclusion:: Spending Score for Male is greater than for female
######################################clustering algorithms###################
#kmeans:
input_data <- df
km_data <- kmeans(input_data,centers = 3, nstart = 20)
km_data
# Cluster identification for
# each observation
km_data$center
#Plot the clusters and their centres.
plot(df$Annual.Income..k..,df$Spending.Score..1.100., col = km_data$cluster)
#divided data to 3 clusters accourding Spending.Score &Annual.Income
###conclusion:: we found people who have Annual.Income more than 60 Spending.Score
#less than 40 or more than 80
#and found people who have Annual.Income less than 60 Spending.Score gather in [40:60]
#####dbscan algo#######################
customer_prep<-df[3:4]
customer_prep<-scale(customer_prep)
#eps_plot <- kNNdistplot(customer_prep, k=3)
#eps_plot<- abline(h = 0.45, lty = 2)
set.seed(50)
d <- dbscan(customer_prep, eps = 0.45, minPts =  2)
d$cluster
d
plot(multishapes,main="DBSCAN",col=d$cluster)
################Hierarchical Clustering###############################
# Finding distance matrix
distance_mat <- dist(df, method = 'euclidean')
distance_mat
# Fitting Hierarchical clustering Model
# to training dataset
set.seed(240)  # Setting seed
Hierar_cl <- hclust(distance_mat,method = "average")
Hierar_cl
# Plotting dendrogram
plot(Hierar_cl)
# Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
# Cutting tree by no. of clusters
fit <- cutree(Hierar_cl, k = 3 )
fit
table(fit)
rect.hclust(Hierar_cl, k = 3, border = "green")
###conclusion::Hierarchical Clustering divided data into 3 clusters
################################classification algorithms######################
#naive Bayes
# Splitting data into train & test
split <- sample.split(df2, SplitRatio = 0.8)
train_cl_b <- subset(df2, split == "TRUE")
test_cl_b <- subset(df2, split == "FALSE")
# Fitting Naive Bayes Model
# to training dataset
set.seed(120)  # Setting Seed
classifier_cl <- naiveBayes(spend_score_range ~ ., data = train_cl_b)
classifier_cl
# Predicting on test data'
y_pred <- predict(classifier_cl, newdata = test_cl_b)
y_pred
# Confusion Matrix
cm <- table(test_cl_b$spend_score_range, y_pred)
cm
#######################################################
#Decision Tree
#split into train and test data
sample_data = sample.split(df2, SplitRatio = 0.8)
train_data <- subset(df2, sample_data == TRUE)
test_data <- subset(df2, sample_data == FALSE)
model<- ctree(spend_score_range ~ ., train_data)
plot(model)
#predict
predict_model<-predict(model, test_data)
m_at <- table(test_data$spend_score_range, predict_model)
m_at
#calculate accuracy
ac_Test <- sum(diag(m_at)) / sum(m_at)
print(paste('Accuracy for test is found to be',ac_Test))
############################################################
#KNN
# Splitting data into train
# and test data
split_k <- sample.split(df2, SplitRatio = 0.8)
train_cl_k <- subset(df2, split_k == TRUE)
test_cl_k <- subset(df2, split_k == FALSE)
# to training dataset
classifier_knn <- knn(train = train_cl_k ,test = test_cl_k,cl = train_cl_k$Gender, k = 1)
# Confusiin Matrix
cm <- table(test_cl_k$Gender, classifier_knn)
cm
# Model Evaluation - Choosing K
# Calculate out of Sample error
misClassError <- mean(classifier_knn != test_cl_k$Gender)
print(paste('Accuracy =', 1-misClassError))
# K = 3
classifier_knn <- knn(train = train_cl_k ,test = test_cl_k,cl = train_cl_k$Gender, k = 3)
misClassError <- mean(classifier_knn != test_cl_k$Gender)
print(paste('Accuracy =', 1-misClassError))
# K = 5
classifier_knn <- knn(train = train_cl_k ,test = test_cl_k,cl = train_cl_k$Gender, k = 5)
misClassError <- mean(classifier_knn != test_cl_k$Gender)
print(paste('Accuracy =', 1-misClassError))
# K = 7
classifier_knn <- knn(train = train_cl_k ,test = test_cl_k,cl = train_cl_k$Gender, k = 7)
misClassError <- mean(classifier_knn != test_cl_k$Gender)
print(paste('Accuracy =', 1-misClassError))
################################################################
#Another Kmeans Clustering
Kmeans_DF=Kmeans_DF[Kmeans_DF$Age>=18,]
#keep data with income more than or equal 0
Kmeans_DF=Kmeans_DF[Kmeans_DF$Annual.Income..k..>=0,]
#keaap data with spending score more than or equal 1 (delete any score row less than 1)
Kmeans_DF=Kmeans_DF[Kmeans_DF$Spending.Score..1.100.>=1,]
#keap data with spending score less than or equal 100 ((delete any score row more than 100))
Kmeans_DF=Kmeans_DF[Kmeans_DF$Spending.Score..1.100.<=100,]
#create column to spend score range (low, med,high)
Kmeans_DF[, 'spend_score_range'] = NA
Kmeans_DF$spend_score_range[Kmeans_DF$Spending.Score..1.100.<35]<-"low"
Kmeans_DF$spend_score_range[Kmeans_DF$Spending.Score..1.100.>=35&Kmeans_DF$Spending.Score..1.100.<70]<-"med"
Kmeans_DF$spend_score_range[Kmeans_DF$Spending.Score..1.100.>=70]<-"high"
data<-Kmeans_DF
#Kmeans 2 (clustering) between age and annual income
rdata<-data[,2:3]
tempdata<-scale(rdata)
distData<-dist(tempdata)
#to get the best number of clusters using elbow method (here the best number of clusters is 5 because after number 5 the number of squares decrease too slow)
fviz_nbclust(tempdata,kmeans,method = "wss")+labs(subtitle = "Elbow method")
km.out<-kmeans(tempdata,centers = 5,nstart = 100)
#visualize the clustering algorithm results
km.cluster<-km.out$cluster
#name each point to be unique
rownames(tempdata)<- paste(data$spend_score_range,1:dim(data)[1],sep = "_")
# to display the clusters
fviz_cluster(list(data=tempdata,cluster=km.cluster))
#display objects in each cluster
table(Kmeans_DF$spend_score_range,km.cluster)
table(Kmeans_DF$Gender,km.cluster)
#K means 3 (clustering) spending score and annual income
rdata<-data[,3:4]
#tempdata<-rdata
tempdata<-scale(rdata)
distData<-dist(tempdata)
#to get the best number of clusters using elbow method (here the best number of clusters is 6 because after number 6 the number of squares decrease too slow)
fviz_nbclust(tempdata,kmeans,method = "wss")+labs(subtitle = "Elbow method")
km.out<-kmeans(tempdata,centers = 6,nstart = 100)
#visualize the clustering algorithm results
km.cluster<-km.out$cluster
#set the name of each row with unique name for example("male_1","male_2")
rownames(tempdata)<- paste(data$Gender,1:dim(data)[1],sep = "_")
# to display the clusters
fviz_cluster(list(data=tempdata,cluster=km.cluster))
table(Kmeans_DF$spend_score_range,km.cluster)
table(Kmeans_DF$Gender,km.cluster)
#visualize gender and and annual income and age
#(Blue for male) and (red for female)
#relationship between income and spend score (conclusion)->
plot(x=data$Annual.Income..k..,y=data$Spending.Score..1.100.,xlab = "Annnual income",ylab = "Spending Score",col=ifelse(data$Gender=="Male",'blue','red'),pch=19,main = "income and spending")
#relationship between age and spend score  (conclusion)->the less ages are higher spending score
plot(x=data$Age,y=data$Spending.Score..1.100.,xlab = "Age",ylab = "Spending Score",col=ifelse(data$Gender=="Male",'blue','red'),pch=19,main = "age and spending")
#relationship between age and spend score  (conclusion)->the ages between 30 and 50 have the highest income
plot(x=data$Age,y=data$Annual.Income..k..,xlab = "Age",ylab = "annual income",col=ifelse(data$Gender=="Male",'blue','red'),pch=19,main = "age and annual income",)
#count number of males and females
maleCount<-length(which(df2$Gender=="Male"))
femaleCount<-length(which(df2$Gender=="Female"))
#store it into list(combine)
gendercount<-c(maleCount,femaleCount)
genderLabel<-c("Male","Female")
#get the percentage of each gender
GenderPercent<- round(100 * gendercount / sum(gendercount), 1)
#display the pie chart and percentage
pie(gendercount,label=GenderPercent, main = "Gender pie chart",, col = rainbow(length(gendercount)))
#display the color of gender
legend("topright",genderLabel,
cex = 0.9, fill = rainbow(length(gendercount)))
#read data set and put in data frame
df<-read.csv("CustomerSegmentation.csv")
#summary about the data set
str(df)
#drop id CustomerID we don't need it
df <- subset(df , select = -c(CustomerID))
#check if exist missing values
complete.cases(df)
#to remove any record contain missing values
#df<- df[complete.cases(df), ]
#check if exist duplicated rows
duplicated(df)
#to remove duplicated rows
df<-df[!duplicated(df), ]
#Get A copy of Clean Data Frame To Use it in end of file without transforming Gender Column
Kmeans_DF<-df
#convert categorical variables to numirc
df$Gender=(factor(df$Gender , level =c("Male" , "Female") , labels =c(0,1) ))
df=df[df$Age>=18,]
#keep data with income more than or equal 0
df=df[df$Annual.Income..k..>=0,]
#keaap data with spending score more than or equal 1 (delete any score row less than 1)
df=df[df$Spending.Score..1.100.>=1,]
#keap data with spending score less than or equal 100 ((delete any score row more than 100))
df=df[df$Spending.Score..1.100.<=100,]
#exist outlier in Annual.Income column
#outlier = any value > (Q3+1.5*IQR) or any value < (Q1-1.5*IQR)
boxplot(df)
#boxplot(df$Annual.Income..k..)$out
#find Q1, Q3, and interquartile range for values in column Annual.Income
Q1 <- quantile(df$Annual.Income..k.., .25)
Q3 <- quantile(df$Annual.Income..k.., .75)
IQR <- IQR(df$Annual.Income..k..)
#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3
df <- subset(df, df$Annual.Income..k..> (Q1 - 1.5*IQR) & df$Annual.Income..k..< (Q3 + 1.5*IQR))
input_data <- df
km_data <- kmeans(input_data,centers = 3, nstart = 20)
km_data
# Cluster identification for
# each observation
km_data$center
#Plot the clusters and their centres.
plot(df$Annual.Income..k..,df$Spending.Score..1.100., col = km_data$cluster)
#divided data to 3 clusters accourding Spending.Score &Annual.Income
###conclusion:: we found people who have Annual.Income more than 60 Spending.Score
#less than 40 or more than 80
#and found people who have Annual.Income less than 60 Spending.Score gather in [40:60]
# df data frame for clustering algorithms
#df2  data frame for classification algorithms
df2 <- df
#create column to spend score range (low, med,high)
df2[, 'spend_score_range'] = NA
df2$spend_score_range[df2$Spending.Score..1.100.<35]<-"low"
df2$spend_score_range[df2$Spending.Score..1.100.>=35&df2$Spending.Score..1.100.<70]<-"med"
df2$spend_score_range[df2$Spending.Score..1.100.>=70]<-"high"
spend_unique<-unique(df2$spend_score_range
#convert categorical data to numeric
df2$spend_score_range<-as.numeric(factor(df2$spend_score_range,levels = spend_unique))
#to drop spending score from data frame that using for classification algorithms
df2 <- subset(df2 , select = -c(Spending.Score..1.100.))
#to remove duplicated rows
df<-df[!duplicated(df), ]
#setwd('E:/third year material/firstSemester/Statistical Inference/Team_code')
#read data set and put in data frame
df<-read.csv("CustomerSegmentation.csv")
#summary about the data set
str(df)
#drop id CustomerID we don't need it
df <- subset(df , select = -c(CustomerID))
#check if exist missing values
complete.cases(df)
#to remove any record contain missing values
#df<- df[complete.cases(df), ]
#check if exist duplicated rows
duplicated(df)
#to remove duplicated rows
df<-df[!duplicated(df), ]
#setwd('E:/third year material/firstSemester/Statistical Inference/Team_code')
#read data set and put in data frame
df<-read.csv("CustomerSegmentation.csv")
#summary about the data set
str(df)
#drop id CustomerID we don't need it
df <- subset(df , select = -c(CustomerID))
#check if exist missing values
complete.cases(df)
#to remove any record contain missing values
#df<- df[complete.cases(df), ]
#check if exist duplicated rows
duplicated(df)
#to remove duplicated rows
df<-df[!duplicated(df), ]
#Get A copy of Clean Data Frame To Use it in end of file without transforming Gender Column
Kmeans_DF<-df
#convert categorical variables to numirc
df$Gender=(factor(df$Gender , level =c("Male" , "Female") , labels =c(0,1) ))
#keep data with age more than or equal 18
df=df[df$Age>=18,]
#keep data with income more than or equal 0
df=df[df$Annual.Income..k..>=0,]
#keaap data with spending score more than or equal 1 (delete any score row less than 1)
df=df[df$Spending.Score..1.100.>=1,]
#keap data with spending score less than or equal 100 ((delete any score row more than 100))
df=df[df$Spending.Score..1.100.<=100,]
#exist outlier in Annual.Income column
#outlier = any value > (Q3+1.5*IQR) or any value < (Q1-1.5*IQR)
boxplot(df)
q()
q()
